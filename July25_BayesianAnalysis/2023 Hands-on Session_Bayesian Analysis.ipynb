{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023 JETSCAPE Summer School Hands-on Session (Bayesian Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from Derek Everett's Jupyter notebook for Band Camp 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this Notebook\n",
    "\n",
    "In this notebook we will explore topics related to performing **Bayesian inference** (in particular **Bayesian parameter estimation**) using a **Gaussian Process** (GP) emulator for our underlying model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "**Bayesian Inference** is a statistical methodology which can handle arbitrarily complicated problems of inference. \n",
    "\n",
    "Suppose we know some information $D$, for example a set of experimental measurements. \n",
    "Now, suppose we want to make an inference about a parameter $\\theta$, for example some physical property of system which can not be directly measured. \n",
    "\n",
    "Bayes' rule can be written as $$p(\\theta|D) = \\frac{ p(D|\\theta) p(\\theta) }{ p(D) },$$ \n",
    "\n",
    "where\n",
    "- $p(\\theta|D)$ is our \"posterior\" for $\\theta$ given that $D$ is realized, \n",
    "\n",
    "- $p(D|\\theta)$ is the \"likelihood\" of observing $D$ given the parameter $\\theta$, \n",
    "\n",
    "- $p(\\theta)$ is our \"prior\" belief about $\\theta$ before observing $D$, \n",
    "\n",
    "- $p(D)$ is the \"evidence\". \n",
    "\n",
    "If we are only interested in $\\theta$, then we can use that the evidence is independent of $\\theta$ and solve instead the proportionality\n",
    "$$p(\\theta|D) \\propto p(D|\\theta) p(\\theta) .$$ \n",
    "This fact will be very useful for Bayesian parameter estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our simulator\n",
    "\n",
    "We see that our likelihood function $p(D|\\theta)$ encodes the conditional probability of observing some value of the data given a particular value of the model parameter.\n",
    "\n",
    "In this notebook, we assume there is a *linear* relationship between model parameter $\\theta$ and output variable $y$, and the simulation model is given in `lin_model` (see code below).\n",
    "\n",
    "Note that we use a linear model in this notebook because it isn't computationally demanding, allowing us to focus on concepts of Bayesian inference and emulation. However, whenever we discuss our model, we should have in mind a realistic physics model can be arbitrarily complex, non-linear and slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Model Error\n",
    "\n",
    "Let's add to our linear model statistical (uncorrelated) error on top of every output. \n",
    "This will be useful for understanding how any statistical model errors influence our inference problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressing the model\n",
    "Let $y$ denote the ouput, and $\\theta$ the parameter of interest. We can write our linear model by\n",
    "$$y = m \\cdot \\theta + b +\\epsilon,$$ \n",
    "which has a slope $m$, intercept $b$ and statistical error $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #useful for math operations\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "import seaborn as sns #pretty plots\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "\n",
    "import GPy # Library with Gaussian Processes\n",
    "from sklearn.preprocessing import StandardScaler #useful for scaling data\n",
    "\n",
    "import emcee #for performing Markov Chain Monte Carlo\n",
    "import corner #for plotting the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function will define our simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise level controls statistical scatter in our simulator $\\epsilon$\n",
    "noise = 0.01 #amount of statistical scatter in our training calculations\n",
    "np.random.seed(1)\n",
    "\n",
    "def lin_model(theta, intercept = 0.12, slope = -0.25, noise = noise):\n",
    "    \"\"\"This function will play the role of a \n",
    "    realistic physics model. Here it is a linear model \n",
    "    with an additional random noise error.\"\"\"\n",
    "    \n",
    "    y = intercept + slope * (theta) # the mean model prediction\n",
    "    dy = noise * np.random.normal() #the sampled model statistical error\n",
    "    y += dy #add the model stat. error to the model mean\n",
    "    y = np.max([0., y]) #suppose our measurement is positive\n",
    "    return y, dy\n",
    "\n",
    "lin_model = np.vectorize(lin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a fast Model Emulator for slow simulators \n",
    "\n",
    "A realistic physics model, for example, could take hours to run a single event given some value for our parameter. Furthermore, we may need thousands of events to calculate our observable with sufficient statistical precision. \n",
    "\n",
    "For computationally demanding models we can employ a fast surrogate which can estimate the interpolation uncertainty.\n",
    "\n",
    "We use Gaussian process for this purpose in this notebook. Gaussian process is especially useful because they provide non-parametric interpolations (as opposed to a polynomial fit, for example).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like any interpolation, we need a sampling of points in our parameter space where we know the **simulator output**. \n",
    "\n",
    "So, we first run our simulation on a sampling of points that **fill our parameter space** and call this sample our **design points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_design_pts = 20 # this sets the number of design points where we will run our linear model\n",
    "theta_min = 0. # this defines a minimum value for our parameter\n",
    "theta_max = 1. / np.pi # this defines a maximum value for our parameter\n",
    "\n",
    "#this chooses our sample to be a regular grid, which is an efficient sampling in one dimension\n",
    "#it is reshaped into a 2D array so that we can readily used\n",
    "#in various libraries\n",
    "model_X = np.linspace(theta_min, theta_max, n_design_pts).reshape(-1,1)\n",
    "\n",
    "#these are the outputs of our linear model, assuming that the model has finite statistical error\n",
    "model_y, model_dy = lin_model(model_X)\n",
    "\n",
    "#lets plot our simulator outputs\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "plt.errorbar(model_X.flatten(), model_y.flatten(), model_dy.flatten(), fmt='o', c='black')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simulator Outputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "1. Does it look like a linear model produced these data?\n",
    "2. Try playing with the amount of noise (error) in these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training our Gaussian Process (GP)\n",
    "We will use a Gaussian process (https://en.wikipedia.org/wiki/Gaussian_process) to interpolate between the design points.\n",
    "\n",
    "For an intuitive feeling for how they work, play with this widget http://www.tmpl.fi/gp/.\n",
    "For more details see http://www.gaussianprocess.org/gpml/chapters/RW.pdf.\n",
    "\n",
    "A Gaussian process is defined with some choice of a **kernel function**. \n",
    "Please see this page for a brief explanation of a few popular kernels : https://www.cs.toronto.edu/~duvenaud/cookbook/.\n",
    "\n",
    "This is a very good visual exploration of Gaussian processes as well as different kernel functions: https://distill.pub/2019/visual-exploration-gaussian-processes/.\n",
    "\n",
    "\n",
    "We will tell our GP library the kernel function to use, and typically some guidance for the range of the hyperparameters. Then when we call the `optimize()` operation, the library will find the values of hyperparameters that maximize a likelihood function:\n",
    "\n",
    "$$\\log p(y^*|y_{t}, \\theta) \\propto -\\frac{1}{2}y_{t}^{T} \\Sigma^{-1}_{y_t} y_{t} - \\frac{1}{2} \\log |\\Sigma_{y_t}|,$$\n",
    "\n",
    "where $\\Sigma_{y_t}$ is the covariance matrix resulting from applying the covariance function to the **training data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will define and train a GP\n",
    "\n",
    "We will use a combination of a **Squared Exponential Kernel** and a **White Noise Kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the 'size' of possible variation of our parameters\n",
    "ptp = max(model_X) - min(model_X)\n",
    "\n",
    "#This is our Squared Exponential Kernel\n",
    "rbf_kern = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=ptp)\n",
    "\n",
    "#This is a white noise kernel, \n",
    "#necessary because our simulator has finite statistical accuracy,\n",
    "#and therefore statistical scatter in its predictions\n",
    "white_kern = GPy.kern.White(1, variance=noise)\n",
    "\n",
    "#The total kernel function is a sum here, \n",
    "#but other combinations are possible (products, compositions, ...)\n",
    "my_kernel = (rbf_kern + white_kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with many machine learning toolkits, out-of-the-box performance is often best when we first scale our outputs to unit range. The 'Standard Scaler' (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) is convenient for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first scale our observables \n",
    "model_y_copy = model_y.copy()\n",
    "scaler = StandardScaler(copy=True).fit(model_y_copy)\n",
    "scaled_model_y = scaler.transform(model_y, copy=True) # the scaled model outputs (zero mean and unit variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our GP on the linear model calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our Gaussian process model, and fit it to the linear model calculations\n",
    "my_gp = GPy.models.GPRegression(model_X, scaled_model_y, my_kernel)\n",
    "my_gp.optimize(messages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the fitted parameters\n",
    "print(my_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing an 'emulator'\n",
    "It's useful to define a method which handles both the scaling of our observables as well as the prediction with the GP. We call this function the **emulator**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emu_predict(theta):\n",
    "    \"\"\"This function handles the scaling and GP interpolation together, \n",
    "    returning our prediction in the ordinary observable space\n",
    "    rather than the scaled observable space.\n",
    "    This map is what we call our 'emulator'. \"\"\"\n",
    "    X = theta.reshape(-1, 1)\n",
    "    scaled_y, scaled_dy2 = my_gp.predict(X, full_cov=False)\n",
    "    scaled_dy2 = scaled_dy2[:,0] #change shape of the variance\n",
    "    scaled_dy = np.sqrt(scaled_dy2) #GPy returns the variance, want std. dev.\n",
    "    y = scaler.inverse_transform(scaled_y).reshape(len(theta)) #scale back to original scale: essentially scaled_y*scaler.scale_+scaler.mean_\n",
    "    dy = scaled_dy * scaler.scale_\n",
    "    return y, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how well our emulator fits the simulator\n",
    "\n",
    "and if its behavior is sensible/interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a regular grid to plot our emulator predictions\n",
    "n_plot_pts = 100\n",
    "gp_X_plot = np.linspace(theta_min, theta_max, n_plot_pts)\n",
    "\n",
    "#get the GP emulator's predictions of both the mean and std. deviation\n",
    "gp_y, gp_dy = emu_predict(gp_X_plot)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8,6))\n",
    "plt.plot(gp_X_plot, gp_y, color='red', label='GP mean')\n",
    "plt.fill_between(gp_X_plot, y1 = gp_y - 2.*gp_dy, y2 = gp_y + 2.*gp_dy, \n",
    "                 interpolate=True, alpha=0.7, label=r'GP 2$\\sigma$', color='orange')\n",
    "plt.fill_between(gp_X_plot, y1 = gp_y - gp_dy, y2 = gp_y + gp_dy, \n",
    "                 interpolate=True, alpha=0.7, label=r'GP 1$\\sigma$', color='blue')\n",
    "plt.errorbar(model_X.flatten(), model_y.flatten(), model_dy.flatten(), fmt='o', c='black', label='Simulator Output')\n",
    "\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('y')\n",
    "plt.title('GP Emulator and Training Points')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "1. Examine how increasing/decreasing the number of design points affects the mean and std. dev. of the GP emulator prediction. Does it fit your expectation?\n",
    "\n",
    "2. Examine how increasing/decreasing the model statistical error affects the mean and std. dev. of the GP emulator prediction. Does it fit your expectation?\n",
    "\n",
    "3. Examine how changing the distribution of design points affects the mean and std. dev. of the GP emulator prediction (try a design which has regions which are sparsely populated by design points). Does it fit your expectation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We expect our emulator to fit the points on which it was trained\n",
    "...our definition of the GP likelihood function is designed to do just that!\n",
    "\n",
    "Ultimately, we want to know if we can trust our emulator's predictions for points in parameter space at which it was **not trained**. \n",
    "\n",
    "So, let's perform some validations of our GP emulator, using a **novel testing set** of model calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this defines a new set of points in parameter space where we will run our linear model\n",
    "n_test_pts = 100\n",
    "model_X_test = np.random.uniform(theta_min, theta_max, n_test_pts).reshape(-1,1)\n",
    "#get the linear model outputs for these new points\n",
    "model_y_test, model_dy_test = lin_model(model_X_test)\n",
    "\n",
    "#Now use the emulator trained only on the **original design points** to predict\n",
    "#outputs on the **new testing set**\n",
    "gp_y_test, gp_dy_test = emu_predict(model_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the emulator prediction vs the linear model output\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "plt.xlabel(r'Linear model $y$ output with error')\n",
    "plt.ylabel(r'Emulator $y$ prediction with error')\n",
    "model_y_test_min = np.min(model_y_test)\n",
    "model_y_test_max = np.max(model_y_test)\n",
    "perfect_line = np.linspace(0.8*model_y_test_min, 1.2*model_y_test_max, 100)\n",
    "plt.plot(perfect_line, perfect_line, color='r', label='perfect', ls=':', lw=2)\n",
    "plt.errorbar(model_y_test.flatten(), gp_y_test, xerr=model_dy_test.flatten(), yerr=gp_dy_test.flatten(), \n",
    "             ls='None', marker='o', alpha=0.7)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the performance look? \n",
    "There are stricter tests we can use to check if our emulator predictions are biased.\n",
    "\n",
    "If $\\hat{y}(\\theta)$ is our emulator prediction for the parameters $\\theta$, and $y(\\theta)$ is our linear model output, we can define the **residual** $\\hat{y}(\\theta) - y(\\theta)$.\n",
    "\n",
    "Let's plot the residual as a function of $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "model_y_test = model_y_test.reshape(n_test_pts)\n",
    "res = gp_y_test - model_y_test # calculate the residuals \n",
    "axes[0].scatter(model_X_test, res)\n",
    "axes[0].set_xlabel(r'$\\theta$')\n",
    "axes[0].set_ylabel(r'$\\hat{y} - y$')\n",
    "axes[1].hist(res)\n",
    "axes[1].set_xlabel(r'$\\theta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the prediction look biased?\n",
    "\n",
    "By inspection, does it look like our emulator has significant bias some values of $\\theta$?\n",
    "\n",
    "They are even more illuminating tests one can make, for example Quantile-Quantile plots (https://en.wikipedia.org/wiki/Q–Q_plot). You can explore this on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Bayesian Inference\n",
    "Now, we have a fast and accurate emulator that we can trust to compare to data anywhere in the parameter space. \n",
    "\n",
    "So let's use our emulator to perform **Bayesian inference**.\n",
    "\n",
    "Recall, that our **posterior** $p(\\theta|D)$ of our parameters $\\theta$ given the observed experimental data $D$ is the product of our **prior** belief about the parameters $p(\\theta)$ and the **likelihood** $p(D|\\theta)$ of observing those experimental data given the true value of the parameters is $\\theta$. This is Bayes' Rule:\n",
    "\n",
    "$$p(\\theta|D) \\propto p(D|\\theta)p(\\theta).$$\n",
    "\n",
    "So, before using experimental data to update our belief about $\\theta$, we need to define our prior belief about $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing and plotting our priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define two different priors, so we can examine the effect that our prior has on our posterior.\n",
    "\n",
    "One prior will be flat between two limits. The other prior will be informed by a belief, before seeing our data, that the parameter $\\theta$ is more likely to be a certain value within these limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define two different priors, one more informed than the other\n",
    "\n",
    "#a flat prior\n",
    "def log_flat_prior(theta):\n",
    "    \"\"\"Flat prior on value between limits\"\"\"\n",
    "    if (theta_min < theta) and (theta < theta_max):\n",
    "        return 0. # log(1)\n",
    "    else:\n",
    "        return -np.inf  # log(0)\n",
    "    \n",
    "#a peaked prior\n",
    "prior_peak = 2. / (4. * np.pi)  # the value of theta we belief most likely, before seeing data\n",
    "prior_width = 1. / (10. * np.pi) #our uncertainty about this value, before seeing the data\n",
    "def log_peaked_prior(theta):\n",
    "    \"\"\"Peaked (Gaussian) prior on value between limits\"\"\"\n",
    "    if (theta_min < theta) and (theta < theta_max):\n",
    "        return -0.5 * (theta - prior_peak)**2. / prior_width**2. \n",
    "    else:\n",
    "        return -np.inf  # log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets plot our two priors by sampling them, and plotting their histograms\n",
    "n_samples_prior = int(1e6)\n",
    "samples_flat_prior = np.random.uniform(theta_min, theta_max, n_samples_prior) \n",
    "samples_peaked_prior =  np.random.normal( prior_peak, prior_width, n_samples_prior) \n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "plt.hist(samples_flat_prior, label='Flat prior', alpha=0.5, density=True, color='blue', bins=50)\n",
    "plt.hist(samples_peaked_prior, label='Peaked prior', alpha=0.5, density=True, color='red', bins=50)\n",
    "plt.xlim([theta_min, theta_max])\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta)$')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Likelihood\n",
    "To compare our model predictions with experiment, we need to define our likelihood function.\n",
    "The likelihood is a model for the conditional probability of observing the data given some true value of the parameters. In this specific example, it models\n",
    "the conditional probability of observing some experimental value for $y$ given some value of $\\theta$.\n",
    "\n",
    "A commonplace assumption is that the experimental errors follow a multivariate Gaussian distribution. This distribution also maximizes the informational entropy subject to the constraints of being normalizable, having a known mean, and a known variance.\n",
    "For details see (https://github.com/furnstahl/Physics-8805/blob/master/topics/maximum-entropy/MaxEnt.ipynb). \n",
    "\n",
    "The normal likelihood function is probably a good assumption for our problem, because of the nature of the measurement. However, one should consider if this normal likelihood function is appropriate depending on the nature of the specific problem and measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, y_exp, dy_exp):\n",
    "    '''Log-likelihood for data y_exp and uncertainties \n",
    "        dy_exp given parameter array theta.'''\n",
    "    #use our GP emulator to approximate the linear model\n",
    "    y_pred, dy_pred = emu_predict(theta) # emulation prediction and uncertainty\n",
    "    sigma2 = dy_pred**2. + dy_exp**2. #total variance, emulation and exp.\n",
    "    return -0.5*np.log(2.*np.pi*sigma2) -0.5*( (y_pred - y_exp)**2 / sigma2  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "1. Why does the total uncertainty `sigma2` in the likelihood function have this expression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Posterior \n",
    "The posterior is the product of the prior and likelihood function. \n",
    "\n",
    "It follows that the logarithm of the posterior is the sum of the logs of the prior and likelihood.\n",
    "\n",
    "Since we have two different choices of our prior, we will have two different posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posterior using flat prior\n",
    "def log_posterior_flat_prior(theta, y_exp, dy_exp):\n",
    "    '''Log posterior for data y_exp and uncertainties \n",
    "        dy_exp given parameter array theta, assuming prior is flat.'''\n",
    "    return log_flat_prior(theta) + log_likelihood(theta, y_exp, dy_exp)\n",
    "\n",
    "#posterior using peaked prior\n",
    "def log_posterior_peaked_prior(theta, y_exp, dy_exp):\n",
    "    '''Log posterior for data y_exp and uncertainties \n",
    "        dy_exp given parameter array theta, assuming prior is Gaussian.'''\n",
    "    return log_peaked_prior(theta) + log_likelihood(theta, y_exp, dy_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring the value of $\\theta$ using experimental data\n",
    "\n",
    "Suppose that an experiment measures $y$, and is reported by a mean value and uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_exp = 0.09 # experimental data mean\n",
    "dy_exp = 0.01 # experimental data uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our current problem is much-simplified by the use of a linear model, in general we will have no analytic expression for our likelihood function. In this case, one needs a set of numerical tools which can approximate the likelihood function. \n",
    "\n",
    "In addition, for many problems of interest our parameter space can be high-dimensional, so these methods need to work well for high-dimensional problems.\n",
    "\n",
    "We approach both of these problems by employing Markov Chain Monte Carlo sampling (http://www.columbia.edu/~mh2078/MachineLearningORFE/MCMC_Bayes.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we will use a python implementation called *emcee* (https://emcee.readthedocs.io/en/stable/), which will work well for our simple purposes. There are much more sophisticated algorithms for estimating the posterior today; e.g., parallel-tempered MCMC: https://emcee.readthedocs.io/en/v2.2.1/user/pt/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are some general settings for the MCMC\n",
    "ndim = 1  # number of parameters in the model\n",
    "nwalkers = 20*ndim  # number of MCMC walkers\n",
    "nburn = 1000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 2000  # number of MCMC steps to take after the burn-in period finished\n",
    "nthin = 5 # thinning steps (keep every nthin-th iteration)\n",
    "\n",
    "# we'll start at random locations within the prior volume\n",
    "starting_guesses = theta_min + \\\n",
    "                   (theta_max - theta_min) * np.random.rand(nwalkers,ndim)\n",
    "\n",
    "####Sampling the posterior with a flat prior####\n",
    "print(\"Sampling Posterior with Flat Prior...\")\n",
    "print(\"MCMC sampling using emcee (affine-invariant ensemble sampler) with {0} walkers\".format(nwalkers))\n",
    "sampler_flat_prior = emcee.EnsembleSampler(nwalkers, ndim, log_posterior_flat_prior, args=[y_exp, dy_exp])\n",
    "# \"burn-in\" period; save final positions and then reset\n",
    "state = sampler_flat_prior.run_mcmc(starting_guesses, nburn)\n",
    "sampler_flat_prior.reset()\n",
    "# production sampling period\n",
    "sampler_flat_prior.run_mcmc(state.coords, nsteps, thin=nthin)\n",
    "# discard burn-in points, perform thinning and flatten the walkers; the shape of samples is (nwalkers*nsteps, ndim)\n",
    "posterior_flat_prior = sampler_flat_prior.chain.reshape((-1, ndim))\n",
    "\n",
    "####Sampling the posterior with a peaked prior####\n",
    "print(\"Sampling Posterior with Peaked Prior...\")\n",
    "print(\"MCMC sampling using emcee (affine-invariant ensemble sampler) with {0} walkers\".format(nwalkers))\n",
    "sampler_peaked_prior = emcee.EnsembleSampler(nwalkers, ndim, log_posterior_peaked_prior, args=[y_exp, dy_exp])\n",
    "# \"burn-in\" period; save final positions and then reset\n",
    "state = sampler_peaked_prior.run_mcmc(starting_guesses, nburn)\n",
    "sampler_peaked_prior.reset()\n",
    "# production sampling period\n",
    "sampler_peaked_prior.run_mcmc(state.coords, nsteps, thin=nthin)\n",
    "# discard burn-in points, perform thinning and flatten the walkers; the shape of samples is (nwalkers*nsteps, ndim)\n",
    "posterior_peaked_prior = sampler_peaked_prior.chain.reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC Diagnostics\n",
    "\n",
    "We can plot the traceplots of the posterior samples.\n",
    "\n",
    "More information on MCMC diagnostics can be found here (and in other modules), feel free to explore them later:\n",
    "- https://pypi.org/project/arviz/\n",
    "- https://pymc3-testing.readthedocs.io/en/rtd-docs/api/diagnostics.html\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traceplots\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(posterior_flat_prior)\n",
    "plt.title('Flat Prior')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(posterior_peaked_prior)\n",
    "plt.title('Peaked Prior')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traceplots show that the MCMC samplers are mixing well: the samples explore (move around) the parameter space rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting our Posteriors\n",
    "\n",
    "We can plot the samples of our posterior as histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.histplot(posterior_flat_prior.flatten(), kde=True, \n",
    "             color='blue', label='Posterior w/ Flat Prior',stat='density')\n",
    "sns.histplot(posterior_peaked_prior.flatten(), kde=True,\n",
    "             color='red', label='Posterior w/ Peaked Prior',stat='density')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta | y$[exp]$)$')\n",
    "plt.yticks([])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. What do you notice is different about the two posteriors above (their medians, their uncertainties, etc...)?\n",
    "2. Try reducing the experimental error on our measurement. What do you expect to happen, and what happens? How does it depend on our emulation (interpolation) uncertainty?\n",
    "3. Try playing with the parameters which defined the 'peaked' prior (e.g. reducing/increasing its width). What happens?\n",
    "4. In the case where we use the flat prior, what is the relation between the posterior and the likelihood function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We should also plot our posterior predictive distribution\n",
    "\n",
    "Our posterior in parameter space $p(\\theta|D)$ induces a probability distribution in the space of our observables, through our model function.\n",
    "\n",
    "We can use our emulator to predict the value of the observable $y$ for each sample of the posterior of $\\theta$, and plot this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_posterior_pred_flat, dy_posterior_pred_flat = emu_predict(posterior_flat_prior) #posterior_flat_prior contains posterior samples of $\\theta$ using flat prior\n",
    "y_posterior_pred_peak, dy_posterior_pred_peak = emu_predict(posterior_peaked_prior) #posterior_peaked_prior contains posterior samples of $\\theta$ using peaked prior\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.histplot(y_posterior_pred_flat, kde=True, \n",
    "             color='blue', label='Posterior Pred. w/ Flat Prior',stat='density')\n",
    "sns.histplot(y_posterior_pred_peak, kde=True, \n",
    "             color='red', label='Posterior Pred. w/ Peaked Prior',stat='density')\n",
    "plt.yticks([])\n",
    "plt.xlabel(r'$y$')\n",
    "plt.axvspan(y_exp-dy_exp, y_exp+dy_exp, alpha=0.3, color='g', label=r'Exp. $2\\sigma$')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. What do you notice is different about the two posterior predictive distributions above (their medians, their uncertainties, etc...)?\n",
    "2. What do you conclude about the 'peaked' prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline validation: sampling from Target Posterior distribution:\n",
    "Finally, we will sample from our target posterior distribution, as a baseline validation for previous results. (see Slides 22-24 in the lecture notes)\n",
    "\n",
    "The main difference is that, in this section, we will use the simulator (`lin_model`) to compute likelihood directly, instead of using the GP emulator. \n",
    "\n",
    "Note that this section is just for validating our posterior distributions generated using the GP emulator. In practice, running the simulator can be computationally expensive and it may be infeasible to generate target posterior distribution using the simulator. (That's also the reason why we introduce emulation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Likelihood using simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_simulator(theta, y_exp, dy_exp):\n",
    "    '''Log-likelihood for data y_exp and uncertainties \n",
    "        dy_exp given parameter array theta.'''\n",
    "    #use the simulator\n",
    "    y_sim, dy_sim = lin_model(theta) # output of simulator\n",
    "    sigma2 = dy_exp**2. # exp. variance\n",
    "    return -0.5*np.log(2.*np.pi*sigma2) -0.5*( (y_sim - y_exp)**2 / sigma2  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Posterior using simulator\n",
    "The posterior is the product of the prior and likelihood function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posterior using flat prior\n",
    "def log_posterior_flat_prior_simulator(theta, y_exp, dy_exp):\n",
    "    '''Log posterior for data y_exp and uncertainties \n",
    "        dy_exp given parameter array theta, assuming prior is flat.'''\n",
    "    return log_flat_prior(theta) + log_likelihood_simulator(theta, y_exp, dy_exp)\n",
    "\n",
    "#posterior using peaked prior\n",
    "def log_posterior_peaked_prior_simulator(theta, y_exp, dy_exp):\n",
    "    '''Log posterior for data y_exp and uncertainties \n",
    "        dy_exp given parameter array theta, assuming prior is Gaussian.'''\n",
    "    return log_peaked_prior(theta) + log_likelihood_simulator(theta, y_exp, dy_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC\n",
    "\n",
    "Same set up, just changing the log posterior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are some general settings for the MCMC\n",
    "ndim = 1  # number of parameters in the model\n",
    "nwalkers = 20*ndim  # number of MCMC walkers\n",
    "nburn = 1000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 2000  # number of MCMC steps to take after the burn-in period finished\n",
    "nthin = 5 # thinning steps (keep every nthin-th iteration)\n",
    "\n",
    "# we'll start at random locations within the prior volume\n",
    "starting_guesses = theta_min + \\\n",
    "                   (theta_max - theta_min) * np.random.rand(nwalkers,ndim)\n",
    "\n",
    "####Sampling the posterior with a flat prior####\n",
    "print(\"Sampling Posterior with Flat Prior...\")\n",
    "print(\"MCMC sampling using emcee (affine-invariant ensemble sampler) with {0} walkers\".format(nwalkers))\n",
    "sampler_flat_prior_simulator = emcee.EnsembleSampler(nwalkers, ndim, log_posterior_flat_prior_simulator, args=[y_exp, dy_exp])\n",
    "# \"burn-in\" period; save final positions and then reset\n",
    "state = sampler_flat_prior_simulator.run_mcmc(starting_guesses, nburn)\n",
    "sampler_flat_prior_simulator.reset()\n",
    "# production sampling period\n",
    "sampler_flat_prior_simulator.run_mcmc(state.coords, nsteps, thin=nthin)\n",
    "# discard burn-in points, perform thinning and flatten the walkers; the shape of samples is (nwalkers*nsteps, ndim)\n",
    "posterior_flat_prior_simulator = sampler_flat_prior_simulator.chain.reshape((-1, ndim))\n",
    "\n",
    "####Sampling the posterior with a peaked prior####\n",
    "print(\"Sampling Posterior with Peaked Prior...\")\n",
    "print(\"MCMC sampling using emcee (affine-invariant ensemble sampler) with {0} walkers\".format(nwalkers))\n",
    "sampler_peaked_prior_simulator = emcee.EnsembleSampler(nwalkers, ndim, log_posterior_peaked_prior_simulator, args=[y_exp, dy_exp])\n",
    "# \"burn-in\" period; save final positions and then reset\n",
    "state = sampler_peaked_prior_simulator.run_mcmc(starting_guesses, nburn)\n",
    "sampler_peaked_prior_simulator.reset()\n",
    "# production sampling period\n",
    "sampler_peaked_prior_simulator.run_mcmc(state.coords, nsteps, thin=nthin)\n",
    "# discard burn-in points, perform thinning and flatten the walkers; the shape of samples is (nwalkers*nsteps, ndim)\n",
    "posterior_peaked_prior_simulator = sampler_peaked_prior_simulator.chain.reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Target Posteriors and compare to Posteriors using Emulator\n",
    "\n",
    "We can plot the samples of target posterior (using simulator) and our posterior (using emulator) together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(posterior_flat_prior_simulator.flatten(), kde=True, \n",
    "             color='blue', label='Target Posterior w/ Flat Prior',stat='density')\n",
    "sns.histplot(posterior_flat_prior.flatten(), kde=True,\n",
    "             color='red', label='Posterior (GP Emulator) w/ Flat Prior',stat='density')\n",
    "\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta | y$[exp]$)$')\n",
    "plt.yticks([])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(posterior_peaked_prior_simulator.flatten(), kde=True, \n",
    "             color='blue', label='Target Posterior w/ Peaked Prior',stat='density')\n",
    "sns.histplot(posterior_peaked_prior.flatten(), kde=True,\n",
    "             color='red', label='Posterior (GP Emulator) w/ Peaked Prior',stat='density')\n",
    "\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta | y$[exp]$)$')\n",
    "plt.yticks([])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Do posterior distributions using emulator (our Bayesian inference results) agree with the target posterior distributions using simulator (baseline validations)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're finished!\n",
    "\n",
    "Now you can try applying some of these ideas to your own research. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
